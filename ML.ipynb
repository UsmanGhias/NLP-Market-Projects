{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1y2418lIOsz90EFoYbRJSFJr9n-As3GyC",
      "authorship_tag": "ABX9TyOOEocWFiiHczWi68uiZOUp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UsmanGhias/NLP-Market-Projects/blob/main/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Exploring the Limits of Language Models: Human vs. AI-Generated Text\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qZMD3-TN06K7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Text Classification: Develop a classification model capable of accurately distinguishing between human-written text and AI-generated text. This model is not just a tool for analysis but also a stepping stone toward understanding the intricacies of language models like GPT-3 and ChatGPT.\n",
        "- Text Generation: Experiment with N-gram models to generate sentences that closely resemble human writing, exploring how changes in model parameters affect the coherence and authenticity of the generated text."
      ],
      "metadata": {
        "id": "2VuETgG42DT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM5Gn8bKF_06",
        "outputId": "7c12ee48-5110-42ab-e339-70f3bcfc7cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical Calculation for N-gram Model with Laplacian Smoothing"
      ],
      "metadata": {
        "id": "tpZBgk0lMmwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Introduction to N-grams and Laplacian Smoothing:\n",
        "\n",
        "- N-grams are sequences of 'n' items (words or letters) used for predicting the next item in a sequence. For example, in a trigram model (n=3), we look at sequences of three words.\n",
        "- Laplacian (or Add-One) Smoothing is used to handle the issue of zero probabilities for unseen n-grams in the training data. By adding one to the count of each n-gram during probability calculations, we ensure that every potential n-gram has a non-zero chance of occurring."
      ],
      "metadata": {
        "id": "JC0oi2M5Mmp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example: Calculating Probability with Laplace Smoothing"
      ],
      "metadata": {
        "id": "SC1ED4A9MmkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Let suppose we want to calculate the probability of the bigram \"the program\" using Laplace smoothing from provided text. (I copied Text from gpt.txt file and hum.txt file)"
      ],
      "metadata": {
        "id": "oJhsGftqMmf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Count the Occurrences\n",
        "\n",
        "- Let \\(C(\"the program\")\\) be the count of the bigram \"the program\" in corpus. Assume \\(C(\"the program\") = 2\\).\n",
        "- Let \\(C(\"the\")\\) be the count of the word \"the\" preceding our target bigram. Assume \\(C(\"the\") = 10\\).\n",
        "- Let \\(V\\) be the vocabulary size, which is the total number of unique words in the corpus. Assume \\(V = 100\\).\n",
        "\n",
        "#### Step 2: Apply Laplace Smoothing Formula\n",
        "\n",
        "The formula for calculating the probability of a word \\(w_i\\) given the previous word \\(w_{i-1}\\) with Laplace smoothing (add-one smoothing) is:\n",
        "\n",
        "```Latex\n",
        "\\[ P(w_i | w_{i-1}) = \\frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + V} \\]\n",
        "```\n",
        "\n",
        "- Substituting our counts into the formula gives:\n",
        "\n",
        "```Latex\n",
        "\\[ P(\"program\" | \"the\") = \\frac{C(\"the program\") + 1}{C(\"the\") + V} = \\frac{2 + 1}{10 + 100} = \\frac{3}{110} \\]\n",
        "```\n",
        "\n",
        "#### Complete Explanation\n",
        "\n",
        "- The numerator \\(C(\"the program\") + 1\\) represents the smoothed count of the bigram \"the program\". We add 1 to ensure that every bigram, including those not seen in the training corpus, has a non-zero probability.\n",
        "- The denominator \\(C(\"the\") + V\\) represents the smoothed count of all bigrams starting with \"the\". We add \\(V\\) (the vocabulary size) to account for the possibility of any word from the vocabulary following \"the\".\n",
        "\n",
        "### Example Results\n",
        "\n",
        "The calculated probability \\(P(\"program\" | \"the\") = \\frac{3}{110}\\) indicates that, after applying Laplace smoothing, the bigram \"the program\" has a small but non-zero probability of occurring in our corpus. This method ensures that even unseen bigrams can be accounted for in our language model, which is crucial for handling new or rare word combinations in natural language processing tasks.\n",
        "\n",
        "\n",
        "```latex\n",
        "P(\"program\" | \"the\") = \\frac{C(\"the program\") + 1}{C(\"the\") + V} = \\frac{2 + 1}{10 + 100} = \\frac{3}{110}\n",
        "```\n"
      ],
      "metadata": {
        "id": "k-tE-FHyMmbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "AtVG-ERDGCo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "KKQhE0koGcxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from random import choices"
      ],
      "metadata": {
        "id": "RJ-iIBKLF-dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaning Function"
      ],
      "metadata": {
        "id": "RWGWKFeCHGNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans input text by:\n",
        "    - Lowercasing the text\n",
        "    - Removing all non-alphanumeric characters (except periods, exclamation marks, and question marks)\n",
        "    - Adding <START> and <END> tokens to each sentence.\n",
        "    \"\"\"\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove unwanted characters, keeping '.?!' for sentence delimitation\n",
        "    text = re.sub(r'[^\\w\\s.?!]', '', text)\n",
        "    # Split text into sentences and add <START> and <END> tokens\n",
        "    sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
        "    cleaned_sentences = [f'<START> {sentence.strip()} <END>' for sentence in sentences if sentence]\n",
        "    return cleaned_sentences\n"
      ],
      "metadata": {
        "id": "J130v-u5F-ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load and Clean the Dataset"
      ],
      "metadata": {
        "id": "roIdZcXXHRA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's the Function to load and clean our files\n",
        "def load_and_clean_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    cleaned_text = clean_text(text)\n",
        "    return cleaned_text\n",
        "\n",
        "# Paths to our text files\n",
        "hum_file_path = '/content/hum.txt'\n",
        "gpt_file_path = '/content/gpt.txt'\n",
        "\n",
        "# Loading and cleaning the datasets\n",
        "hum_cleaned_texts = load_and_clean_file(hum_file_path)\n",
        "gpt_cleaned_texts = load_and_clean_file(gpt_file_path)\n",
        "\n",
        "# -------------------------------------------------------\n",
        "\n",
        "# Combined the texts from both files if we\n",
        "# cleaned_texts = hum_cleaned_texts + gpt_cleaned_texts\n"
      ],
      "metadata": {
        "id": "TtwrpNUuGE2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(hum_cleaned_texts[:5])\n",
        "print(gpt_cleaned_texts[:5])\n",
        "\n",
        "\n",
        "# # Checking cleaned text\n",
        "# print(cleaned_texts[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbrEV5XfTC7U",
        "outputId": "6f571b7b-264e-43a1-fdef-ca39d43b0cac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<START> neither texas  nor any other state  is allowed to secede from the us . <END>', '<START> why is the answer not so simple as either becoming one country and tearing down the walls or becoming two countries ? <END>', '<START> combusinessnetwortharticlewhengovernmentfinescompanieswhogetscash3189724 . <END>', '<START> just be happy  relax yourselftake care\\nyou could  theoretically  tunnel from na to asia  since they are not antipodal  and thus you would nt need to tunnel through the center of the earth . <END>', '<START> . <END>']\n",
            "['<START> they then used this information to calculate the value of r that would best describe the behavior of gases . <END>', '<START> flash is a software platform that was developed by adobe and is used to create animations  games  and other interactive content . <END>', '<START> the expansion of deserts can have significant impacts on the world . <END>', '<START> if you are concerned about your credit utilization rate  you may want to consider paying down your balances or requesting a credit limit increase to help keep your credit utilization rate low . <END>', '<START> others may try to enter the country without following the proper procedures  such as by crossing the border illegally . <END>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Splited the Data into Training & Testing Sets for ChatGPT and HUMAN"
      ],
      "metadata": {
        "id": "Cd9jTxTPH2hS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def split_data(data, test_size=0.1):\n",
        "    random.shuffle(data)\n",
        "    split_idx = int(len(data) * (1 - test_size))\n",
        "    return data[:split_idx], data[split_idx:]\n"
      ],
      "metadata": {
        "id": "q_mpWwcQ5XLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hum_train_sentences, hum_test_sentences = split_data(hum_cleaned_texts, test_size=0.1)\n",
        "gpt_train_sentences, gpt_test_sentences = split_data(gpt_cleaned_texts, test_size=0.1)\n"
      ],
      "metadata": {
        "id": "86HR9VA15hFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_data(file_path, data):\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        for sentence in data:\n",
        "            file.write(sentence + \"\\n\")\n"
      ],
      "metadata": {
        "id": "mEPv-fLM5miO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths for saving\n",
        "hum_train_path = '/content/hum_train.txt'\n",
        "hum_test_path = '/content/hum_test.txt'\n",
        "gpt_train_path = '/content/gpt_train.txt'\n",
        "gpt_test_path = '/content/gpt_test.txt'\n",
        "\n",
        "# Save the datasets\n",
        "save_data(hum_train_path, hum_train_sentences)\n",
        "save_data(hum_test_path, hum_test_sentences)\n",
        "save_data(gpt_train_path, gpt_train_sentences)\n",
        "save_data(gpt_test_path, gpt_test_sentences)\n"
      ],
      "metadata": {
        "id": "zl3SP6Qe5mcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating N-grams for Human and ChatGPT"
      ],
      "metadata": {
        "id": "An60C3lyTZgg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOj2rOulCW5-"
      },
      "outputs": [],
      "source": [
        "def generate_ngrams(sentences, n=2):\n",
        "    \"\"\"\n",
        "    Generate n-grams from a list of sentences.\n",
        "    \"\"\"\n",
        "    ngrams = []\n",
        "    for sentence in sentences:\n",
        "        tokens = sentence.split()\n",
        "        ngrams.extend(list(zip(*[tokens[i:] for i in range(n)])))\n",
        "    return ngrams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Human"
      ],
      "metadata": {
        "id": "N_VyEmWN6XVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming hum_train_sentences is already defined\n",
        "hum_bigrams = generate_ngrams(hum_train_sentences, n=2)\n",
        "hum_trigrams = generate_ngrams(hum_train_sentences, n=3)\n"
      ],
      "metadata": {
        "id": "6c2MczZzIaPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hum_bigram_freq = Counter(hum_bigrams)\n",
        "hum_trigram_freq = Counter(hum_trigrams)\n"
      ],
      "metadata": {
        "id": "QcQH8Dh96ZcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatGPT"
      ],
      "metadata": {
        "id": "97Z9e1Xg6aVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming gpt_train_sentences is already defined\n",
        "gpt_bigrams = generate_ngrams(gpt_train_sentences, n=2)\n",
        "gpt_trigrams = generate_ngrams(gpt_train_sentences, n=3)\n"
      ],
      "metadata": {
        "id": "FrTpl2FM6gvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_bigram_freq = Counter(gpt_bigrams)\n",
        "gpt_trigram_freq = Counter(gpt_trigrams)\n"
      ],
      "metadata": {
        "id": "GxogTYrT6jdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Sets"
      ],
      "metadata": {
        "id": "BJ9oYLiO7L0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Human Dataset\n",
        "# Vocabulary size\n",
        "human_vocab_size_bigrams = len(set(hum_bigrams))\n",
        "human_vocab_size_trigrams = len(set(hum_trigrams))\n",
        "\n",
        "# Total N-grams\n",
        "total_human_bigrams = sum(hum_bigram_freq.values())\n",
        "total_human_trigrams = sum(hum_trigram_freq.values())\n"
      ],
      "metadata": {
        "id": "Ipa26il-7P7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatGPT Dataset\n",
        "# Vocabulary size\n",
        "gpt_vocab_size_bigrams = len(set(gpt_bigrams))\n",
        "gpt_vocab_size_trigrams = len(set(gpt_trigrams))\n",
        "\n",
        "# Total N-grams\n",
        "total_gpt_bigrams = sum(gpt_bigram_freq.values())\n",
        "total_gpt_trigrams = sum(gpt_trigram_freq.values())\n"
      ],
      "metadata": {
        "id": "WuZWpOIr7ZRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Applying Laplacian Smoothing"
      ],
      "metadata": {
        "id": "u6-c_WPjIzTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Laplacian Smoothing Function"
      ],
      "metadata": {
        "id": "JeDQ5JfHI8G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def laplace_smoothed_probability(ngram, ngram_freq, total_ngrams, vocabulary_size, alpha=1):\n",
        "    \"\"\"\n",
        "    Here we are calculating Laplace-smoothing probability of an n-gram.\n",
        "\n",
        "    # Parameters:\n",
        "    :param ngram: The n-gram for which to calculate the probability.\n",
        "    :param ngram_freq: A Counter object containing frequencies of n-grams.\n",
        "    :param total_ngrams: The total number of n-grams in the training set.\n",
        "    :param vocabulary_size: The size of the vocabulary.\n",
        "    :param alpha: The smoothing parameter (default: 1 for Laplacian smoothing).\n",
        "    :return: The smoothed probability of the n-gram.\n",
        "    \"\"\"\n",
        "    ngram_count = ngram_freq[ngram]\n",
        "    smoothed_prob = (ngram_count + alpha) / (total_ngrams + alpha * vocabulary_size)\n",
        "    return smoothed_prob\n"
      ],
      "metadata": {
        "id": "Xvj04f_XIaI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating Vocabulary Size"
      ],
      "metadata": {
        "id": "ZDRKPZzKI_R9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Human Dataset Vocabulary Size and Total N-grams\n",
        "human_vocab_size_bigrams = len(set(hum_bigrams))\n",
        "human_vocab_size_trigrams = len(set(hum_trigrams))\n",
        "total_human_bigrams = sum(hum_bigram_freq.values())\n",
        "total_human_trigrams = sum(hum_trigram_freq.values())\n",
        "\n",
        "# ChatGPT Dataset Vocabulary Size and Total N-grams\n",
        "gpt_vocab_size_bigrams = len(set(gpt_bigrams))\n",
        "gpt_vocab_size_trigrams = len(set(gpt_trigrams))\n",
        "total_gpt_bigrams = sum(gpt_bigram_freq.values())\n",
        "total_gpt_trigrams = sum(gpt_trigram_freq.values())\n"
      ],
      "metadata": {
        "id": "-u_RsRQ4I4oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P3tzYknoc7Xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_word_human(current_tokens, backoff_order=[2, 1]):\n",
        "    return get_next_word_general(current_tokens, hum_bigram_freq, hum_trigram_freq, backoff_order, human_vocab_size_bigrams, total_human_bigrams)\n",
        "\n",
        "def get_next_word_gpt(current_tokens, backoff_order=[2, 1]):\n",
        "    return get_next_word_general(current_tokens, gpt_bigram_freq, gpt_trigram_freq, backoff_order, gpt_vocab_size_bigrams, total_gpt_bigrams)\n",
        "\n",
        "def get_next_word_general(current_tokens, bigram_freq, trigram_freq, backoff_order, vocab_size, total_ngrams):\n",
        "    for n in backoff_order:\n",
        "        possible_ngrams = None\n",
        "        if n == 2:\n",
        "            possible_ngrams = [(ngram, freq) for ngram, freq in bigram_freq.items() if ngram[:-1] == tuple(current_tokens[-(n-1):])]\n",
        "        elif n == 3 and trigram_freq is not None:\n",
        "            possible_ngrams = [(ngram, freq) for ngram, freq in trigram_freq.items() if ngram[:-1] == tuple(current_tokens[-2:])]\n",
        "\n",
        "        if possible_ngrams:\n",
        "            total_freq = sum(freq for _, freq in possible_ngrams)\n",
        "            choices, weights = zip(*[(ngram[-1], freq / total_freq) for ngram, freq in possible_ngrams])\n",
        "            return random.choices(choices, weights=weights)[0]\n",
        "\n",
        "    return None  # Fallback if no suitable continuation is found\n"
      ],
      "metadata": {
        "id": "5wyIrNGic651"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Model Evaluation"
      ],
      "metadata": {
        "id": "AOlZee_SJI6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining Perplexity Calculation Function"
      ],
      "metadata": {
        "id": "NtGfjljrJMK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_perplexity(sentences, ngram_freq, total_ngrams, vocab_size, n=2, alpha=1):\n",
        "\n",
        "    \"\"\"\n",
        "    Calculating perplexity of a set of sentences given an n-gram model with Laplace smoothing\n",
        "    using Libraries: math.log & math.exp:\n",
        "\n",
        "    # Parameters:\n",
        "    :param sentences: The set of sentences to evaluate.\n",
        "    :param ngram_freq: A Counter object containing frequencies of n-grams.\n",
        "    :param total_ngrams: The total number of n-grams in the training data.\n",
        "    :param vocab_size: The size of the vocabulary.\n",
        "    :param n: The order of the n-gram model (2 for bigram, 3 for trigram).\n",
        "    :param alpha: The smoothing parameter.\n",
        "    :return: The calculated perplexity.\n",
        "    \"\"\"\n",
        "\n",
        "    log_prob_sum = 0\n",
        "    N = 0\n",
        "    for sentence in sentences:\n",
        "        tokens = ['<START>'] + sentence.split() + ['<END>']\n",
        "        sentence_ngrams = zip(*[tokens[i:] for i in range(n)])\n",
        "        for ngram in sentence_ngrams:\n",
        "            prob = laplace_smoothed_probability(ngram, ngram_freq, total_ngrams, vocab_size, alpha)\n",
        "            log_prob_sum += math.log(prob)\n",
        "            N += 1\n",
        "    perplexity = math.exp(-log_prob_sum / N)\n",
        "    return perplexity\n",
        "\n"
      ],
      "metadata": {
        "id": "cjBr6KOkJFLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating using Bigram & Trigram Model on the Test Set"
      ],
      "metadata": {
        "id": "08e14lOaJTdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Human Dataset\n",
        "perplexity_human_bigrams = calculate_perplexity(hum_test_sentences, hum_bigram_freq, total_human_bigrams, human_vocab_size_bigrams, n=2)\n",
        "perplexity_human_trigrams = calculate_perplexity(hum_test_sentences, hum_trigram_freq, total_human_trigrams, human_vocab_size_trigrams, n=3)\n",
        "\n",
        "# ChatGPT Dataset\n",
        "perplexity_gpt_bigrams = calculate_perplexity(gpt_test_sentences, gpt_bigram_freq, total_gpt_bigrams, gpt_vocab_size_bigrams, n=2)\n",
        "perplexity_gpt_trigrams = calculate_perplexity(gpt_test_sentences, gpt_trigram_freq, total_gpt_trigrams, gpt_vocab_size_trigrams, n=3)\n"
      ],
      "metadata": {
        "id": "JK43MmaZJQqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print perplexity of Human Dataset and ChatGPT data set\n",
        "\n",
        "print(\"Human Dataset:\")\n",
        "print(f\"  - Bigram Perplexity: {perplexity_human_bigrams}\")\n",
        "print(f\"  - Trigram Perplexity: {perplexity_human_trigrams}\")\n",
        "\n",
        "print(\"ChatGPT Dataset:\")\n",
        "print(f\"  - Bigram Perplexity: {perplexity_gpt_bigrams}\")\n",
        "print(f\"  - Trigram Perplexity: {perplexity_gpt_trigrams}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ldi5UCl-Azz",
        "outputId": "b243110b-a386-46b6-c70c-987c0cb0f1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human Dataset:\n",
            "  - Bigram Perplexity: 125213.95266966379\n",
            "  - Trigram Perplexity: 1697687.723540955\n",
            "ChatGPT Dataset:\n",
            "  - Bigram Perplexity: 61908.717378033034\n",
            "  - Trigram Perplexity: 716206.9919634318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Text Generation"
      ],
      "metadata": {
        "id": "9qiQ_KhcJZsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Human Text Generation\n"
      ],
      "metadata": {
        "id": "I68mQ1BRJcw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_sentence(ngram_freq, start_token='<START>', end_token='<END>', n=2):\n",
        "    \"\"\"\n",
        "    Generates a sentence using an n-gram model.\n",
        "\n",
        "    :param ngram_freq: A Counter object containing frequencies of n-grams.\n",
        "    :param start_token: The token indicating the start of a sentence.\n",
        "    :param end_token: The token indicating the end of a sentence.\n",
        "    :param n: The order of the n-gram model.\n",
        "    :return: A generated sentence as a string.\n",
        "    \"\"\"\n",
        "    sentence = [start_token]\n",
        "    while True:\n",
        "        if n == 2:\n",
        "            # For bigrams, consider the last word in the sentence for the next word prediction\n",
        "            current_token = sentence[-1]\n",
        "            possible_ngrams = [(ngram, freq) for ngram, freq in ngram_freq.items() if ngram[0] == current_token]\n",
        "        elif n == 3:\n",
        "            # For trigrams, consider the last two words in the sentence for the next word prediction\n",
        "            if len(sentence) < 2:\n",
        "                # If the sentence has fewer than 2 words, revert to bigram prediction for the next word\n",
        "                current_tokens = (start_token, sentence[-1])\n",
        "            else:\n",
        "                current_tokens = tuple(sentence[-2:])\n",
        "            possible_ngrams = [(ngram, freq) for ngram, freq in ngram_freq.items() if ngram[:2] == current_tokens]\n",
        "        else:\n",
        "            raise ValueError(\"This function currently supports only bigram (n=2) and trigram (n=3) models.\")\n",
        "\n",
        "        # If no possible n-grams are found, break the loop\n",
        "        if not possible_ngrams:\n",
        "            break\n",
        "\n",
        "        total_freq = sum(freq for _, freq in possible_ngrams)\n",
        "        choices, weights = zip(*[(ngram[-1], freq / total_freq) for ngram, freq in possible_ngrams])\n",
        "        next_word = random.choices(choices, weights=weights)[0]\n",
        "\n",
        "        if next_word == end_token or len(sentence) > 100:  # Prevent infinite loops\n",
        "            break\n",
        "        sentence.append(next_word)\n",
        "\n",
        "    return ' '.join(sentence[1:])  # Skip the <START> token for output\n"
      ],
      "metadata": {
        "id": "7CezOC80JXHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate a Sentence, Bigram Laplace Smoothing"
      ],
      "metadata": {
        "id": "DwT3OQaEJkPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_human_sentence_bigram = generate_sentence(hum_bigram_freq, n=2)\n",
        "print(f\"Generated Human Bigram Sentence: {generated_human_sentence_bigram}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH9gXK_ZJiY5",
        "outputId": "aae193e8-a689-4a54-90e9-04b5ae4e2117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Human Bigram Sentence: not only because about 22 you can tell us needs to be a collie lab .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_human_sentence_trigram = generate_sentence_with_trigram_backoff('<START>', '<END>', hum_bigram_freq, hum_trigram_freq, max_length=100)\n",
        "print(f\"Generated Human Trigram Sentence with Backoff: {generated_human_sentence_trigram}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhtrnkmf-oeR",
        "outputId": "31df4ba9-1e6e-4cd5-d95f-06187f61ed37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Human Trigram Sentence with Backoff: other banksinsurance companiesservice providers so popular .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT Text Generation"
      ],
      "metadata": {
        "id": "Wf87p0gz_A-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_gpt_sentence_bigram = generate_sentence(gpt_bigram_freq, n=2)\n",
        "print(f\"Generated ChatGPT Bigram Sentence: {generated_gpt_sentence_bigram}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HdyG8I4_CSf",
        "outputId": "e3dd672f-691e-4ab0-8713-405d6f773456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated ChatGPT Bigram Sentence: the price of the way the joint to understand how a better to make it .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_gpt_sentence_trigram = generate_sentence_with_trigram_backoff('<START>', '<END>', gpt_bigram_freq, gpt_trigram_freq, max_length=100)\n",
        "print(f\"Generated ChatGPT Trigram Sentence with Backoff: {generated_gpt_sentence_trigram}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMEWHMJ7_GnQ",
        "outputId": "f38dab21-6f18-4bf2-93b6-a49c8273f384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated ChatGPT Trigram Sentence with Backoff: scientists have also been used to train their troops overseas to support the cheeks and body in many different countries and it contains sensitive personal information to answer your question it is normal to have legal and illegal while he was a student at harvard university .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WpOMit6X_JLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "srX_s4Km_vYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Human-Written Text Generation\n",
        "Bigram Sentence: \"not only because about 22 you can tell us needs to be a collie lab.\"\n",
        "\n",
        "- Analysis: This sentence, generated from a bigram model, demonstrates a somewhat disjointed structure. It starts coherently but quickly becomes less logical and lacks a clear direction. This illustrates a common limitation of bigram models: reliance on the immediate preceding word often results in less coherent long-term structure.\n",
        "Trigram Sentence with Backoff: \"other banksinsurance companiesservice providers so popular.\"\n",
        "\n",
        "- Analysis: The trigram-generated sentence, while short, suggests a slightly more coherent structure than the bigram sentence. However, the lack of spaces between words (\"banksinsurance\" and \"companiesservice\") indicates a possible preprocessing or tokenization issue. Despite this, the trigram model appears to maintain a more topic-focused approach.\n",
        "\n",
        "# ChatGPT-Generated Text Generation\n",
        "Bigram Sentence: \"the price of the way the joint to understand how a better to make it.\"\n",
        "\n",
        "- Analysis: Similar to the human bigram sentence, this sentence lacks coherence, with a repetitive structure that doesn't logically progress. It reflects the bigram model's limitations in capturing longer dependency relationships within text.\n",
        "\n",
        "Trigram Sentence with Backoff: \"scientists have also been used to train their troops overseas to support the cheeks and body in many different countries and it contains sensitive personal information to answer your question it is normal to have legal and illegal while he was a student at harvard university.\"\n",
        "\n",
        "- Analysis: This sentence showcases a higher degree of coherence and complexity, characteristic of trigram models' ability to capture longer-term dependencies. However, it eventually diverges into a less coherent narrative, suggesting that while trigram models can generate more structured beginnings, maintaining topic consistency over long sentences remains challenging."
      ],
      "metadata": {
        "id": "cpLiivxM_yrn"
      }
    }
  ]
}